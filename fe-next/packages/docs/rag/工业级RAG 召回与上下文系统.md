### 第一部分：原文转录

#### 1. 核心处理模块详解与 RAG 流程图

（对应流程图内容） 检索增强生成模型 RAG (Retrieval-Augmented Generation)
Pipeline： 数据提取 -> 分块 -> Embedding (向量化) -> 索引 -> 向量数据库 <-> 查询
-> 检索 -> 上下文 (Reranker 重排序) -> Prompt 模板 -> 大模型 (LLM) -> 结果返回。

**查询优化模块** 功能描述：对原始用户查询进行语义增强和扩展，提升检索覆盖率。
处理策略：

#### 2. 查询优化具体策略

- 查询改写：生成多个语义相同但表述不同的查询变体。
- 假设性回答：基于问题生成可能的回答，反向推导相关关键词。
- 语义扩展：利用同义词、相关概念扩展查询语义空间。
- 意图识别：识别用户真实意图，针对性优化查询表达。

输入输出：

- 输入：原始用户查询
- 输出：3-5 个优化后的查询变体

**3.2 混合检索模块**
功能描述：结合向量相似度检索和标量精确过滤，实现精准召回。需要先使用 embedding
嵌入模型把问题转成向量进行相似度计算。

#### 3. 配置参数与结果后处理

配置参数：

- 向量相似度阈值：0.6-0.8（可调）
- 最大召回数量：50-100 条（可调）
- 标量过滤条件：基于业务需求动态配置

**结果后处理模块** 功能描述：对初步召回结果进行质量优化。 处理流程：

1. 合并去重 1.1. 基于内容哈希值去除完全重复项 1.2.
   基于语义相似度合并高度相关内容（阈值：0.9）

#### 4. 重排序与质量过滤

2. 重排序 (Rerank) 2.1. 使用专用重排序模型（如 bge-reranker）进行精细排序 2.2.
   考虑因素：语义相关性、时效性、权威性、完整性
3. 质量过滤 3.1. 移除低质量片段（长度过短、格式混乱等） 3.2. 确保最终结果多样性

**对话记忆管理模块** 功能描述：维护多轮对话的上下文连贯性。 记忆管理策略：

#### 5. 记忆策略与上下文管理

- 滑动窗口：保持最近 N 轮对话（默认 N=10）
- 关键信息提取：从历史对话中提取实体、意图、决策等关键信息
- 记忆压缩：当记忆超长时，自动生成摘要替代原始内容
- 重要性衰减：基于时间衰减和重要性评分管理记忆保留

**上下文管理模块** 功能描述：优化提示词构建，防止上下文窗口溢出。 上下文组成：
[系统提示词] [对话记忆摘要] [当前用户问题] [召回的相关文档] [生成要求与约束]

优化策略：

#### 6. 异常处理与降级策略

- 记忆冲突：检测到新旧记忆矛盾时，启动记忆一致性校验
- 上下文超限：自动触发上下文压缩或请求用户简化问题

**降级策略**

- 检索降级：放宽过滤条件或使用关键词检索作为备选
- 记忆降级：临时禁用长期记忆，仅使用短期对话上下文
- 生成降级：切换至轻量级模型或提供标准回复模板

**总结**
数据召回系统通过多阶段的精细化处理，实现了从海量知识库中精准检索相关信息的能力。结合先进的对话记忆管理和上下文优化技术，确保了多轮对话的连贯性和生成质量。系统具备良好的可扩展性和可配置性，能够根据不同业务场景进行灵活调整和优化。

---

### 第二部分：核心架构总结（面试知识库）

这套内容描述的是一个标准的、工业级的 RAG
召回与上下文系统。相比于简单的“向量搜索”，它增加了很多工程化的中间层。

我将其总结为 RAG 进阶的 **漏斗模型**，你可以用这个逻辑来组织你的技术认知：

#### 1. 第一层：查询理解 (Query Understanding) —— 解决“问得不好”的问题

用户的问题往往是模糊的。系统没有直接拿去搜，而是做了预处理：

- **Rewrite (改写)**：消除指代不明（把“它”改成“英伟达”）。
- **Expansion (扩展)**：把“手机”扩展为“移动设备”、“智能终端”，增加命中率。
- **HyDE (假设性回答)**：这是一个高级技巧。先让 LLM
  瞎编一个答案，用这个答案去搜，通常比搜问题本身更能找到语义相似的文档。

#### 2. 第二层：混合检索 (Hybrid Retrieval) —— 解决“单腿走路”的问题

- **向量检索 (Vector)**：负责抓取语义相关的（查“苹果”能出“水果”）。
- **标量过滤 (Scalar Filter)**：负责硬指标过滤（只看“2024年”的数据）。
- _面试应用_：你可以说你的项目不仅用了 pgvector，还结合了 SQL 的 Where
  条件进行混合筛选，保证了数据的时效性。

#### 3. 第三层：精细筛选 (Post-Processing) —— 解决“噪音太多”的问题

这是从 Demo 到生产环境的关键一步。

- **去重**：防止多篇相似文章导致上下文重复。
- **Rerank (重排序)**：这是核心。向量检索出的 Top 100 只是粗排，必须用专门的
  Rerank 模型（如 BGE）进行精排，选出 Top 5。
  - _面试金句_：向量检索决定了召回的范围（Recall），而重排序决定了最终给 LLM
    的精度（Precision）。

#### 4. 第四层：动态记忆 (Adaptive Memory) —— 解决“记不住与记太多”的问题

- **滑动窗口**：保鲜，只留最近 N 句。
- **记忆压缩**：将旧的对话“蒸馏”成摘要。
- **时间衰减**：引入时间因子，让旧记忆的权重随时间降低。

#### 5. 第五层：兜底机制 (Fallback) —— 解决“系统挂了”的问题

- 如果向量检索分太低怎么办？降级为关键词搜索。
- 如果 Context 还是太长怎么办？强行截断或二次压缩。
- _面试应用_：提到这一点会显得你非常有工程经验，考虑了系统的鲁棒性（Robustness）。

### 第三部分：面试应答策略

当面试官问你：**你的 RAG 系统是如何优化检索效果的？**

你可以套用上述逻辑，这样回答：

> “为了解决传统 RAG 检索不准和上下文噪声大的问题，我设计了一个多阶段的
> **检索漏斗 (Retrieval Funnel)**：
>
> 首先是 **查询优化阶段**。针对用户模糊的提问，我会利用 LLM 进行 Query
> Rewriting，生成多个语义变体，甚至利用 HyDE 策略来增强语义检索的准确性。
>
> 其次是
> **混合检索阶段**。我没有单纯依赖向量相似度，而是结合了结构化的标量过滤（如时间、标签）。
>
> 最关键的是 **后处理阶段**。对于召回的 Top 50 结果，我引入了 **Rerank
> (重排序)** 机制，利用专门的 Cross-Encoder
> 模型对相关性进行打分，剔除噪音，只保留信噪比最高的 Top 5 给大模型。
>
> 最后，为了保证系统的稳定性，我设计了
> **降级策略**。当语义检索置信度过低时，系统会自动回退到传统的关键词匹配模式，确保用户总能得到有用的反馈。”
